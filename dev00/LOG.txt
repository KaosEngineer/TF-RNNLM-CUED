Creating Grader Model with configuration:
----------------------------------------------------------
BN: False
L1: False
L2: 4e-07
activation_fn: <function relu at 0x10ab7fed8>
initializer: <function xavier_initializer at 0x1196e3050>
model_type: dnn_grader
n_L1: 113
n_hid: 180
n_in: 32636
n_layers: 2
n_not_tied: 0
n_out: 32636
n_z: 10
output_fn: <function relu at 0x10ab7fed8>
----------------------------------------------------------
Learning Rate: 0.010000
Learning Rate Decay: 0.940000
Batch Size: 64
Optimizer: <class 'tensorflow.python.training.adam.AdamOptimizer'>

